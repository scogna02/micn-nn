[
    {
        "label": "Dataset_ETT_hour",
        "importPath": "data_provider.data_loader",
        "description": "data_provider.data_loader",
        "isExtraImport": true,
        "detail": "data_provider.data_loader",
        "documentation": {}
    },
    {
        "label": "Dataset_ETT_minute",
        "importPath": "data_provider.data_loader",
        "description": "data_provider.data_loader",
        "isExtraImport": true,
        "detail": "data_provider.data_loader",
        "documentation": {}
    },
    {
        "label": "Dataset_Custom",
        "importPath": "data_provider.data_loader",
        "description": "data_provider.data_loader",
        "isExtraImport": true,
        "detail": "data_provider.data_loader",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "diagonal",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "time_features",
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "isExtraImport": true,
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "load_from_tsfile_to_dataframe",
        "importPath": "sktime.datasets",
        "description": "sktime.datasets",
        "isExtraImport": true,
        "detail": "sktime.datasets",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "MICN",
        "importPath": "models",
        "description": "models",
        "isExtraImport": true,
        "detail": "models",
        "documentation": {}
    },
    {
        "label": "data_provider",
        "importPath": "data_provider.data_factory",
        "description": "data_provider.data_factory",
        "isExtraImport": true,
        "detail": "data_provider.data_factory",
        "documentation": {}
    },
    {
        "label": "data_provider",
        "importPath": "data_provider.data_factory",
        "description": "data_provider.data_factory",
        "isExtraImport": true,
        "detail": "data_provider.data_factory",
        "documentation": {}
    },
    {
        "label": "Exp_Basic",
        "importPath": "exp.exp_basic",
        "description": "exp.exp_basic",
        "isExtraImport": true,
        "detail": "exp.exp_basic",
        "documentation": {}
    },
    {
        "label": "Exp_Basic",
        "importPath": "exp.exp_basic",
        "description": "exp.exp_basic",
        "isExtraImport": true,
        "detail": "exp.exp_basic",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "adjust_learning_rate",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "visual",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "adjust_learning_rate",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "visual",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "metric",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "metric",
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "isExtraImport": true,
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "log2",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "ceil",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "TwoStageAttentionLayer",
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "isExtraImport": true,
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "AttentionLayer",
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "isExtraImport": true,
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "FullAttention",
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "isExtraImport": true,
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "torch.fft",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.fft",
        "description": "torch.fft",
        "detail": "torch.fft",
        "documentation": {}
    },
    {
        "label": "math,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math.",
        "description": "math.",
        "detail": "math.",
        "documentation": {}
    },
    {
        "label": "next_fast_len",
        "importPath": "scipy.fftpack",
        "description": "scipy.fftpack",
        "isExtraImport": true,
        "detail": "scipy.fftpack",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "pdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdb",
        "description": "pdb",
        "detail": "pdb",
        "documentation": {}
    },
    {
        "label": "Poly",
        "importPath": "sympy",
        "description": "sympy",
        "isExtraImport": true,
        "detail": "sympy",
        "documentation": {}
    },
    {
        "label": "legendre",
        "importPath": "sympy",
        "description": "sympy",
        "isExtraImport": true,
        "detail": "sympy",
        "documentation": {}
    },
    {
        "label": "Symbol",
        "importPath": "sympy",
        "description": "sympy",
        "isExtraImport": true,
        "detail": "sympy",
        "documentation": {}
    },
    {
        "label": "chebyshevt",
        "importPath": "sympy",
        "description": "sympy",
        "isExtraImport": true,
        "detail": "sympy",
        "documentation": {}
    },
    {
        "label": "eval_legendre",
        "importPath": "scipy.special",
        "description": "scipy.special",
        "isExtraImport": true,
        "detail": "scipy.special",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "torch.nn.modules.linear",
        "description": "torch.nn.modules.linear",
        "isExtraImport": true,
        "detail": "torch.nn.modules.linear",
        "documentation": {}
    },
    {
        "label": "DataEmbedding",
        "importPath": "layers.Embed",
        "description": "layers.Embed",
        "isExtraImport": true,
        "detail": "layers.Embed",
        "documentation": {}
    },
    {
        "label": "TriangularCausalMask",
        "importPath": "utils.masking",
        "description": "utils.masking",
        "isExtraImport": true,
        "detail": "utils.masking",
        "documentation": {}
    },
    {
        "label": "ProbMask",
        "importPath": "utils.masking",
        "description": "utils.masking",
        "isExtraImport": true,
        "detail": "utils.masking",
        "documentation": {}
    },
    {
        "label": "LSHSelfAttention",
        "importPath": "reformer_pytorch",
        "description": "reformer_pytorch",
        "isExtraImport": true,
        "detail": "reformer_pytorch",
        "documentation": {}
    },
    {
        "label": "DataEmbedding",
        "importPath": "layers.layers",
        "description": "layers.layers",
        "isExtraImport": true,
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "series_decomp",
        "importPath": "layers.layers",
        "description": "layers.layers",
        "isExtraImport": true,
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "series_decomp_multi",
        "importPath": "layers.layers",
        "description": "layers.layers",
        "isExtraImport": true,
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "M4Dataset",
        "importPath": "data_provider.m4",
        "description": "data_provider.m4",
        "isExtraImport": true,
        "detail": "data_provider.m4",
        "documentation": {}
    },
    {
        "label": "M4Meta",
        "importPath": "data_provider.m4",
        "description": "data_provider.m4",
        "isExtraImport": true,
        "detail": "data_provider.m4",
        "documentation": {}
    },
    {
        "label": "offsets",
        "importPath": "pandas.tseries",
        "description": "pandas.tseries",
        "isExtraImport": true,
        "detail": "pandas.tseries",
        "documentation": {}
    },
    {
        "label": "to_offset",
        "importPath": "pandas.tseries.frequencies",
        "description": "pandas.tseries.frequencies",
        "isExtraImport": true,
        "detail": "pandas.tseries.frequencies",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Exp_Long_Term_Forecast",
        "importPath": "exp.exp_long_term_forecasting",
        "description": "exp.exp_long_term_forecasting",
        "isExtraImport": true,
        "detail": "exp.exp_long_term_forecasting",
        "documentation": {}
    },
    {
        "label": "Exp_Imputation",
        "importPath": "exp.exp_imputation",
        "description": "exp.exp_imputation",
        "isExtraImport": true,
        "detail": "exp.exp_imputation",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "utils.print_args",
        "description": "utils.print_args",
        "isExtraImport": true,
        "detail": "utils.print_args",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "data_provider",
        "kind": 2,
        "importPath": "data_provider.data_factory",
        "description": "data_provider.data_factory",
        "peekOfCode": "def data_provider(args, flag):\n    Data = data_dict[args.data]\n    timeenc = 0 if args.embed != 'timeF' else 1\n    if flag == 'test':\n        shuffle_flag = False\n        drop_last = True\n        batch_size = 1  # bsz=1 for evaluation\n        freq = args.freq\n    else:\n        shuffle_flag = True",
        "detail": "data_provider.data_factory",
        "documentation": {}
    },
    {
        "label": "data_dict",
        "kind": 5,
        "importPath": "data_provider.data_factory",
        "description": "data_provider.data_factory",
        "peekOfCode": "data_dict = {\n    'ETTh1': Dataset_ETT_hour,\n    'ETTh2': Dataset_ETT_hour,\n    'ETTm1': Dataset_ETT_minute,\n    'ETTm2': Dataset_ETT_minute,\n    'custom': Dataset_Custom,\n}\ndef data_provider(args, flag):\n    Data = data_dict[args.data]\n    timeenc = 0 if args.embed != 'timeF' else 1",
        "detail": "data_provider.data_factory",
        "documentation": {}
    },
    {
        "label": "Dataset_ETT_hour",
        "kind": 6,
        "importPath": "data_provider.data_loader",
        "description": "data_provider.data_loader",
        "peekOfCode": "class Dataset_ETT_hour(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4",
        "detail": "data_provider.data_loader",
        "documentation": {}
    },
    {
        "label": "Dataset_ETT_minute",
        "kind": 6,
        "importPath": "data_provider.data_loader",
        "description": "data_provider.data_loader",
        "peekOfCode": "class Dataset_ETT_minute(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTm1.csv',\n                 target='OT', scale=True, timeenc=0, freq='t', seasonal_patterns=None):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4",
        "detail": "data_provider.data_loader",
        "documentation": {}
    },
    {
        "label": "Dataset_Custom",
        "kind": 6,
        "importPath": "data_provider.data_loader",
        "description": "data_provider.data_loader",
        "peekOfCode": "class Dataset_Custom(Dataset):\n    def __init__(self, root_path, flag='train', size=None,\n                 features='S', data_path='ETTh1.csv',\n                 target='OT', scale=True, timeenc=0, freq='h', seasonal_patterns=None):\n        # size [seq_len, label_len, pred_len]\n        # info\n        if size == None:\n            self.seq_len = 24 * 4 * 4\n            self.label_len = 24 * 4\n            self.pred_len = 24 * 4",
        "detail": "data_provider.data_loader",
        "documentation": {}
    },
    {
        "label": "Exp_Basic",
        "kind": 6,
        "importPath": "exp.exp_basic",
        "description": "exp.exp_basic",
        "peekOfCode": "class Exp_Basic(object):\n    def __init__(self, args):\n        self.args = args\n        self.model_dict = {'MICN': MICN,}\n        self.device = self._acquire_device()\n        self.model = self._build_model().to(self.device)\n    def _build_model(self):\n        raise NotImplementedError\n        return None\n    def _acquire_device(self):",
        "detail": "exp.exp_basic",
        "documentation": {}
    },
    {
        "label": "Exp_Imputation",
        "kind": 6,
        "importPath": "exp.exp_imputation",
        "description": "exp.exp_imputation",
        "peekOfCode": "class Exp_Imputation(Exp_Basic):\n    def __init__(self, args):\n        super(Exp_Imputation, self).__init__(args)\n    def _build_model(self):\n        model = self.model_dict[self.args.model].Model(self.args).float()\n        if self.args.use_multi_gpu and self.args.use_gpu:\n            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n        return model\n    def _get_data(self, flag):\n        data_set, data_loader = data_provider(self.args, flag)",
        "detail": "exp.exp_imputation",
        "documentation": {}
    },
    {
        "label": "Exp_Long_Term_Forecast",
        "kind": 6,
        "importPath": "exp.exp_long_term_forecasting",
        "description": "exp.exp_long_term_forecasting",
        "peekOfCode": "class Exp_Long_Term_Forecast(Exp_Basic):\n    def __init__(self, args):\n        super(Exp_Long_Term_Forecast, self).__init__(args)\n    def _build_model(self):\n        model = self.model_dict[self.args.model].Model(self.args).float()\n        if self.args.use_multi_gpu and self.args.use_gpu:\n            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n        return model\n    def _get_data(self, flag):\n        data_set, data_loader = data_provider(self.args, flag)",
        "detail": "exp.exp_long_term_forecasting",
        "documentation": {}
    },
    {
        "label": "AutoCorrelation",
        "kind": 6,
        "importPath": "layers.AutoCorrelation",
        "description": "layers.AutoCorrelation",
        "peekOfCode": "class AutoCorrelation(nn.Module):\n    \"\"\"\n    AutoCorrelation Mechanism with the following two phases:\n    (1) period-based dependencies discovery\n    (2) time delay aggregation\n    This block can replace the self-attention family mechanism seamlessly.\n    \"\"\"\n    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n        super(AutoCorrelation, self).__init__()\n        self.factor = factor",
        "detail": "layers.AutoCorrelation",
        "documentation": {}
    },
    {
        "label": "AutoCorrelationLayer",
        "kind": 6,
        "importPath": "layers.AutoCorrelation",
        "description": "layers.AutoCorrelation",
        "peekOfCode": "class AutoCorrelationLayer(nn.Module):\n    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n                 d_values=None):\n        super(AutoCorrelationLayer, self).__init__()\n        d_keys = d_keys or (d_model // n_heads)\n        d_values = d_values or (d_model // n_heads)\n        self.inner_correlation = correlation\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.value_projection = nn.Linear(d_model, d_values * n_heads)",
        "detail": "layers.AutoCorrelation",
        "documentation": {}
    },
    {
        "label": "my_Layernorm",
        "kind": 6,
        "importPath": "layers.Autoformer_EncDec",
        "description": "layers.Autoformer_EncDec",
        "peekOfCode": "class my_Layernorm(nn.Module):\n    \"\"\"\n    Special designed layernorm for the seasonal part\n    \"\"\"\n    def __init__(self, channels):\n        super(my_Layernorm, self).__init__()\n        self.layernorm = nn.LayerNorm(channels)\n    def forward(self, x):\n        x_hat = self.layernorm(x)\n        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)",
        "detail": "layers.Autoformer_EncDec",
        "documentation": {}
    },
    {
        "label": "moving_avg",
        "kind": 6,
        "importPath": "layers.Autoformer_EncDec",
        "description": "layers.Autoformer_EncDec",
        "peekOfCode": "class moving_avg(nn.Module):\n    \"\"\"\n    Moving average block to highlight the trend of time series\n    \"\"\"\n    def __init__(self, kernel_size, stride):\n        super(moving_avg, self).__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n    def forward(self, x):\n        # padding on the both ends of time series",
        "detail": "layers.Autoformer_EncDec",
        "documentation": {}
    },
    {
        "label": "series_decomp",
        "kind": 6,
        "importPath": "layers.Autoformer_EncDec",
        "description": "layers.Autoformer_EncDec",
        "peekOfCode": "class series_decomp(nn.Module):\n    \"\"\"\n    Series decomposition block\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(series_decomp, self).__init__()\n        self.moving_avg = moving_avg(kernel_size, stride=1)\n    def forward(self, x):\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean",
        "detail": "layers.Autoformer_EncDec",
        "documentation": {}
    },
    {
        "label": "series_decomp_multi",
        "kind": 6,
        "importPath": "layers.Autoformer_EncDec",
        "description": "layers.Autoformer_EncDec",
        "peekOfCode": "class series_decomp_multi(nn.Module):\n    \"\"\"\n    Multiple Series decomposition block from FEDformer\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(series_decomp_multi, self).__init__()\n        self.kernel_size = kernel_size\n        self.series_decomp = [series_decomp(kernel) for kernel in kernel_size]\n    def forward(self, x):\n        moving_mean = []",
        "detail": "layers.Autoformer_EncDec",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "layers.Autoformer_EncDec",
        "description": "layers.Autoformer_EncDec",
        "peekOfCode": "class EncoderLayer(nn.Module):\n    \"\"\"\n    Autoformer encoder layer with the progressive decomposition architecture\n    \"\"\"\n    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)",
        "detail": "layers.Autoformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "layers.Autoformer_EncDec",
        "description": "layers.Autoformer_EncDec",
        "peekOfCode": "class Encoder(nn.Module):\n    \"\"\"\n    Autoformer encoder\n    \"\"\"\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(Encoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n    def forward(self, x, attn_mask=None):",
        "detail": "layers.Autoformer_EncDec",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "layers.Autoformer_EncDec",
        "description": "layers.Autoformer_EncDec",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    \"\"\"\n    Autoformer decoder layer with the progressive decomposition architecture\n    \"\"\"\n    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None,\n                 moving_avg=25, dropout=0.1, activation=\"relu\"):\n        super(DecoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.self_attention = self_attention\n        self.cross_attention = cross_attention",
        "detail": "layers.Autoformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "layers.Autoformer_EncDec",
        "description": "layers.Autoformer_EncDec",
        "peekOfCode": "class Decoder(nn.Module):\n    \"\"\"\n    Autoformer encoder\n    \"\"\"\n    def __init__(self, layers, norm_layer=None, projection=None):\n        super(Decoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n        self.projection = projection\n    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):",
        "detail": "layers.Autoformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Inception_Block_V1",
        "kind": 6,
        "importPath": "layers.Conv_Blocks",
        "description": "layers.Conv_Blocks",
        "peekOfCode": "class Inception_Block_V1(nn.Module):\n    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n        super(Inception_Block_V1, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_kernels = num_kernels\n        kernels = []\n        for i in range(self.num_kernels):\n            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=2 * i + 1, padding=i))\n        self.kernels = nn.ModuleList(kernels)",
        "detail": "layers.Conv_Blocks",
        "documentation": {}
    },
    {
        "label": "Inception_Block_V2",
        "kind": 6,
        "importPath": "layers.Conv_Blocks",
        "description": "layers.Conv_Blocks",
        "peekOfCode": "class Inception_Block_V2(nn.Module):\n    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n        super(Inception_Block_V2, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_kernels = num_kernels\n        kernels = []\n        for i in range(self.num_kernels // 2):\n            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=[1, 2 * i + 3], padding=[0, i + 1]))\n            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=[2 * i + 3, 1], padding=[i + 1, 0]))",
        "detail": "layers.Conv_Blocks",
        "documentation": {}
    },
    {
        "label": "SegMerging",
        "kind": 6,
        "importPath": "layers.Crossformer_EncDec",
        "description": "layers.Crossformer_EncDec",
        "peekOfCode": "class SegMerging(nn.Module):\n    def __init__(self, d_model, win_size, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.d_model = d_model\n        self.win_size = win_size\n        self.linear_trans = nn.Linear(win_size * d_model, d_model)\n        self.norm = norm_layer(win_size * d_model)\n    def forward(self, x):\n        batch_size, ts_d, seg_num, d_model = x.shape\n        pad_num = seg_num % self.win_size",
        "detail": "layers.Crossformer_EncDec",
        "documentation": {}
    },
    {
        "label": "scale_block",
        "kind": 6,
        "importPath": "layers.Crossformer_EncDec",
        "description": "layers.Crossformer_EncDec",
        "peekOfCode": "class scale_block(nn.Module):\n    def __init__(self, configs, win_size, d_model, n_heads, d_ff, depth, dropout, \\\n                 seg_num=10, factor=10):\n        super(scale_block, self).__init__()\n        if win_size > 1:\n            self.merge_layer = SegMerging(d_model, win_size, nn.LayerNorm)\n        else:\n            self.merge_layer = None\n        self.encode_layers = nn.ModuleList()\n        for i in range(depth):",
        "detail": "layers.Crossformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "layers.Crossformer_EncDec",
        "description": "layers.Crossformer_EncDec",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, attn_layers):\n        super(Encoder, self).__init__()\n        self.encode_blocks = nn.ModuleList(attn_layers)\n    def forward(self, x):\n        encode_x = []\n        encode_x.append(x)\n        for block in self.encode_blocks:\n            x, attns = block(x)\n            encode_x.append(x)",
        "detail": "layers.Crossformer_EncDec",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "layers.Crossformer_EncDec",
        "description": "layers.Crossformer_EncDec",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, self_attention, cross_attention, seg_len, d_model, d_ff=None, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = self_attention\n        self.cross_attention = cross_attention\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.MLP1 = nn.Sequential(nn.Linear(d_model, d_model),\n                                  nn.GELU(),",
        "detail": "layers.Crossformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "layers.Crossformer_EncDec",
        "description": "layers.Crossformer_EncDec",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, layers):\n        super(Decoder, self).__init__()\n        self.decode_layers = nn.ModuleList(layers)\n    def forward(self, x, cross):\n        final_predict = None\n        i = 0\n        ts_d = x.shape[1]\n        for layer in self.decode_layers:\n            cross_enc = cross[i]",
        "detail": "layers.Crossformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Transform",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class Transform:\n    def __init__(self, sigma):\n        self.sigma = sigma\n    @torch.no_grad()\n    def transform(self, x):\n        return self.jitter(self.shift(self.scale(x)))\n    def jitter(self, x):\n        return x + (torch.randn(x.shape).to(x.device) * self.sigma)\n    def scale(self, x):\n        return x * (torch.randn(x.size(-1)).to(x.device) * self.sigma + 1)",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "ExponentialSmoothing",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class ExponentialSmoothing(nn.Module):\n    def __init__(self, dim, nhead, dropout=0.1, aux=False):\n        super().__init__()\n        self._smoothing_weight = nn.Parameter(torch.randn(nhead, 1))\n        self.v0 = nn.Parameter(torch.randn(1, 1, nhead, dim))\n        self.dropout = nn.Dropout(dropout)\n        if aux:\n            self.aux_dropout = nn.Dropout(dropout)\n    def forward(self, values, aux_values=None):\n        b, t, h, d = values.shape",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Feedforward",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class Feedforward(nn.Module):\n    def __init__(self, d_model, dim_feedforward, dropout=0.1, activation='sigmoid'):\n        # Implementation of Feedforward model\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, dim_feedforward, bias=False)\n        self.dropout1 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model, bias=False)\n        self.dropout2 = nn.Dropout(dropout)\n        self.activation = getattr(F, activation)\n    def forward(self, x):",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "GrowthLayer",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class GrowthLayer(nn.Module):\n    def __init__(self, d_model, nhead, d_head=None, dropout=0.1):\n        super().__init__()\n        self.d_head = d_head or (d_model // nhead)\n        self.d_model = d_model\n        self.nhead = nhead\n        self.z0 = nn.Parameter(torch.randn(self.nhead, self.d_head))\n        self.in_proj = nn.Linear(self.d_model, self.d_head * self.nhead)\n        self.es = ExponentialSmoothing(self.d_head, self.nhead, dropout=dropout)\n        self.out_proj = nn.Linear(self.d_head * self.nhead, self.d_model)",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "FourierLayer",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class FourierLayer(nn.Module):\n    def __init__(self, d_model, pred_len, k=None, low_freq=1):\n        super().__init__()\n        self.d_model = d_model\n        self.pred_len = pred_len\n        self.k = k\n        self.low_freq = low_freq\n    def forward(self, x):\n        \"\"\"x: (b, t, d)\"\"\"\n        b, t, d = x.shape",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "LevelLayer",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class LevelLayer(nn.Module):\n    def __init__(self, d_model, c_out, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.c_out = c_out\n        self.es = ExponentialSmoothing(1, self.c_out, dropout=dropout, aux=True)\n        self.growth_pred = nn.Linear(self.d_model, self.c_out)\n        self.season_pred = nn.Linear(self.d_model, self.c_out)\n    def forward(self, level, growth, season):\n        b, t, _ = level.shape",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class EncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, c_out, seq_len, pred_len, k, dim_feedforward=None, dropout=0.1,\n                 activation='sigmoid', layer_norm_eps=1e-5):\n        super().__init__()\n        self.d_model = d_model\n        self.nhead = nhead\n        self.c_out = c_out\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        dim_feedforward = dim_feedforward or 4 * d_model",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n    def forward(self, res, level, attn_mask=None):\n        growths = []\n        seasons = []\n        for layer in self.layers:\n            res, level, growth, season = layer(res, level, attn_mask=None)\n            growths.append(growth)",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "DampingLayer",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class DampingLayer(nn.Module):\n    def __init__(self, pred_len, nhead, dropout=0.1):\n        super().__init__()\n        self.pred_len = pred_len\n        self.nhead = nhead\n        self._damping_factor = nn.Parameter(torch.randn(1, nhead))\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        x = repeat(x, 'b 1 d -> b t d', t=self.pred_len)\n        b, t, d = x.shape",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, c_out, pred_len, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.nhead = nhead\n        self.c_out = c_out\n        self.pred_len = pred_len\n        self.growth_damping = DampingLayer(pred_len, nhead, dropout=dropout)\n        self.dropout1 = nn.Dropout(dropout)\n    def forward(self, growth, season):",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.d_model = layers[0].d_model\n        self.c_out = layers[0].c_out\n        self.pred_len = layers[0].pred_len\n        self.nhead = layers[0].nhead\n        self.layers = nn.ModuleList(layers)\n        self.pred = nn.Linear(self.d_model, self.c_out)\n    def forward(self, growths, seasons):",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "conv1d_fft",
        "kind": 2,
        "importPath": "layers.ETSformer_EncDec",
        "description": "layers.ETSformer_EncDec",
        "peekOfCode": "def conv1d_fft(f, g, dim=-1):\n    N = f.size(dim)\n    M = g.size(dim)\n    fast_len = next_fast_len(N + M - 1)\n    F_f = fft.rfft(f, fast_len, dim=dim)\n    F_g = fft.rfft(g, fast_len, dim=dim)\n    F_fg = F_f * F_g.conj()\n    out = fft.irfft(F_fg, fast_len, dim=dim)\n    out = out.roll((-1,), dims=(dim,))\n    idx = torch.as_tensor(range(fast_len - N, fast_len)).to(out.device)",
        "detail": "layers.ETSformer_EncDec",
        "documentation": {}
    },
    {
        "label": "PositionalEmbedding",
        "kind": 6,
        "importPath": "layers.Embed",
        "description": "layers.Embed",
        "peekOfCode": "class PositionalEmbedding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float()\n                    * -(math.log(10000.0) / d_model)).exp()\n        pe[:, 0::2] = torch.sin(position * div_term)",
        "detail": "layers.Embed",
        "documentation": {}
    },
    {
        "label": "TokenEmbedding",
        "kind": 6,
        "importPath": "layers.Embed",
        "description": "layers.Embed",
        "peekOfCode": "class TokenEmbedding(nn.Module):\n    def __init__(self, c_in, d_model):\n        super(TokenEmbedding, self).__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_in', nonlinearity='leaky_relu')",
        "detail": "layers.Embed",
        "documentation": {}
    },
    {
        "label": "FixedEmbedding",
        "kind": 6,
        "importPath": "layers.Embed",
        "description": "layers.Embed",
        "peekOfCode": "class FixedEmbedding(nn.Module):\n    def __init__(self, c_in, d_model):\n        super(FixedEmbedding, self).__init__()\n        w = torch.zeros(c_in, d_model).float()\n        w.require_grad = False\n        position = torch.arange(0, c_in).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float()\n                    * -(math.log(10000.0) / d_model)).exp()\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)",
        "detail": "layers.Embed",
        "documentation": {}
    },
    {
        "label": "TemporalEmbedding",
        "kind": 6,
        "importPath": "layers.Embed",
        "description": "layers.Embed",
        "peekOfCode": "class TemporalEmbedding(nn.Module):\n    def __init__(self, d_model, embed_type='fixed', freq='h'):\n        super(TemporalEmbedding, self).__init__()\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':",
        "detail": "layers.Embed",
        "documentation": {}
    },
    {
        "label": "TimeFeatureEmbedding",
        "kind": 6,
        "importPath": "layers.Embed",
        "description": "layers.Embed",
        "peekOfCode": "class TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model, embed_type='timeF', freq='h'):\n        super(TimeFeatureEmbedding, self).__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6,\n                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        d_inp = freq_map[freq]\n        self.embed = nn.Linear(d_inp, d_model, bias=False)\n    def forward(self, x):\n        return self.embed(x)\nclass DataEmbedding(nn.Module):",
        "detail": "layers.Embed",
        "documentation": {}
    },
    {
        "label": "DataEmbedding",
        "kind": 6,
        "importPath": "layers.Embed",
        "description": "layers.Embed",
        "peekOfCode": "class DataEmbedding(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n        super(DataEmbedding, self).__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        self.position_embedding = PositionalEmbedding(d_model=d_model)\n        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n            d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n    def forward(self, x, x_mark):",
        "detail": "layers.Embed",
        "documentation": {}
    },
    {
        "label": "FourierBlock",
        "kind": 6,
        "importPath": "layers.FourierCorrelation",
        "description": "layers.FourierCorrelation",
        "peekOfCode": "class FourierBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, seq_len, modes=0, mode_select_method='random'):\n        super(FourierBlock, self).__init__()\n        print('fourier enhanced block used!')\n        \"\"\"\n        1D Fourier block. It performs representation learning on frequency domain, \n        it does FFT, linear transform, and Inverse FFT.    \n        \"\"\"\n        # get modes on frequency domain\n        self.index = get_frequency_modes(seq_len, modes=modes, mode_select_method=mode_select_method)",
        "detail": "layers.FourierCorrelation",
        "documentation": {}
    },
    {
        "label": "FourierCrossAttention",
        "kind": 6,
        "importPath": "layers.FourierCorrelation",
        "description": "layers.FourierCorrelation",
        "peekOfCode": "class FourierCrossAttention(nn.Module):\n    def __init__(self, in_channels, out_channels, seq_len_q, seq_len_kv, modes=64, mode_select_method='random',\n                 activation='tanh', policy=0, num_heads=8):\n        super(FourierCrossAttention, self).__init__()\n        print(' fourier enhanced cross attention used!')\n        \"\"\"\n        1D Fourier Cross Attention layer. It does FFT, linear transform, attention mechanism and Inverse FFT.    \n        \"\"\"\n        self.activation = activation\n        self.in_channels = in_channels",
        "detail": "layers.FourierCorrelation",
        "documentation": {}
    },
    {
        "label": "get_frequency_modes",
        "kind": 2,
        "importPath": "layers.FourierCorrelation",
        "description": "layers.FourierCorrelation",
        "peekOfCode": "def get_frequency_modes(seq_len, modes=64, mode_select_method='random'):\n    \"\"\"\n    get modes on frequency domain:\n    'random' means sampling randomly;\n    'else' means sampling the lowest modes;\n    \"\"\"\n    modes = min(modes, seq_len // 2)\n    if mode_select_method == 'random':\n        index = list(range(0, seq_len // 2))\n        np.random.shuffle(index)",
        "detail": "layers.FourierCorrelation",
        "documentation": {}
    },
    {
        "label": "MultiWaveletTransform",
        "kind": 6,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "class MultiWaveletTransform(nn.Module):\n    \"\"\"\n    1D multiwavelet block.\n    \"\"\"\n    def __init__(self, ich=1, k=8, alpha=16, c=128,\n                 nCZ=1, L=0, base='legendre', attention_dropout=0.1):\n        super(MultiWaveletTransform, self).__init__()\n        print('base', base)\n        self.k = k\n        self.c = c",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "MultiWaveletCross",
        "kind": 6,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "class MultiWaveletCross(nn.Module):\n    \"\"\"\n    1D Multiwavelet Cross Attention layer.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, seq_len_q, seq_len_kv, modes, c=64,\n                 k=8, ich=512,\n                 L=0,\n                 base='legendre',\n                 mode_select_method='random',\n                 initializer=None, activation='tanh',",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "FourierCrossAttentionW",
        "kind": 6,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "class FourierCrossAttentionW(nn.Module):\n    def __init__(self, in_channels, out_channels, seq_len_q, seq_len_kv, modes=16, activation='tanh',\n                 mode_select_method='random'):\n        super(FourierCrossAttentionW, self).__init__()\n        print('corss fourier correlation used!')\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.modes1 = modes\n        self.activation = activation\n    def compl_mul1d(self, order, x, weights):",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "sparseKernelFT1d",
        "kind": 6,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "class sparseKernelFT1d(nn.Module):\n    def __init__(self,\n                 k, alpha, c=1,\n                 nl=1,\n                 initializer=None,\n                 **kwargs):\n        super(sparseKernelFT1d, self).__init__()\n        self.modes1 = alpha\n        self.scale = (1 / (c * k * c * k))\n        self.weights1 = nn.Parameter(self.scale * torch.rand(c * k, c * k, self.modes1, dtype=torch.float))",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "MWT_CZ1d",
        "kind": 6,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "class MWT_CZ1d(nn.Module):\n    def __init__(self,\n                 k=3, alpha=64,\n                 L=0, c=1,\n                 base='legendre',\n                 initializer=None,\n                 **kwargs):\n        super(MWT_CZ1d, self).__init__()\n        self.k = k\n        self.L = L",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "legendreDer",
        "kind": 2,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "def legendreDer(k, x):\n    def _legendre(k, x):\n        return (2 * k + 1) * eval_legendre(k, x)\n    out = 0\n    for i in np.arange(k - 1, -1, -2):\n        out += _legendre(i, x)\n    return out\ndef phi_(phi_c, x, lb=0, ub=1):\n    mask = np.logical_or(x < lb, x > ub) * 1.0\n    return np.polynomial.polynomial.Polynomial(phi_c)(x) * (1 - mask)",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "phi_",
        "kind": 2,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "def phi_(phi_c, x, lb=0, ub=1):\n    mask = np.logical_or(x < lb, x > ub) * 1.0\n    return np.polynomial.polynomial.Polynomial(phi_c)(x) * (1 - mask)\ndef get_phi_psi(k, base):\n    x = Symbol('x')\n    phi_coeff = np.zeros((k, k))\n    phi_2x_coeff = np.zeros((k, k))\n    if base == 'legendre':\n        for ki in range(k):\n            coeff_ = Poly(legendre(ki, 2 * x - 1), x).all_coeffs()",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "get_phi_psi",
        "kind": 2,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "def get_phi_psi(k, base):\n    x = Symbol('x')\n    phi_coeff = np.zeros((k, k))\n    phi_2x_coeff = np.zeros((k, k))\n    if base == 'legendre':\n        for ki in range(k):\n            coeff_ = Poly(legendre(ki, 2 * x - 1), x).all_coeffs()\n            phi_coeff[ki, :ki + 1] = np.flip(np.sqrt(2 * ki + 1) * np.array(coeff_).astype(np.float64))\n            coeff_ = Poly(legendre(ki, 4 * x - 1), x).all_coeffs()\n            phi_2x_coeff[ki, :ki + 1] = np.flip(np.sqrt(2) * np.sqrt(2 * ki + 1) * np.array(coeff_).astype(np.float64))",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "get_filter",
        "kind": 2,
        "importPath": "layers.MultiWaveletCorrelation",
        "description": "layers.MultiWaveletCorrelation",
        "peekOfCode": "def get_filter(base, k):\n    def psi(psi1, psi2, i, inp):\n        mask = (inp <= 0.5) * 1.0\n        return psi1[i](inp) * mask + psi2[i](inp) * (1 - mask)\n    if base not in ['legendre', 'chebyshev']:\n        raise Exception('Base not supported')\n    x = Symbol('x')\n    H0 = np.zeros((k, k))\n    H1 = np.zeros((k, k))\n    G0 = np.zeros((k, k))",
        "detail": "layers.MultiWaveletCorrelation",
        "documentation": {}
    },
    {
        "label": "RegularMask",
        "kind": 6,
        "importPath": "layers.Pyraformer_EncDec",
        "description": "layers.Pyraformer_EncDec",
        "peekOfCode": "class RegularMask():\n    def __init__(self, mask):\n        self._mask = mask.unsqueeze(1)\n    @property\n    def mask(self):\n        return self._mask\nclass EncoderLayer(nn.Module):\n    \"\"\" Compose with two layers \"\"\"\n    def __init__(self, d_model, d_inner, n_head, dropout=0.1, normalize_before=True):\n        super(EncoderLayer, self).__init__()",
        "detail": "layers.Pyraformer_EncDec",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "layers.Pyraformer_EncDec",
        "description": "layers.Pyraformer_EncDec",
        "peekOfCode": "class EncoderLayer(nn.Module):\n    \"\"\" Compose with two layers \"\"\"\n    def __init__(self, d_model, d_inner, n_head, dropout=0.1, normalize_before=True):\n        super(EncoderLayer, self).__init__()\n        self.slf_attn = AttentionLayer(\n            FullAttention(mask_flag=True, factor=0,\n                          attention_dropout=dropout, output_attention=False),\n            d_model, n_head)\n        self.pos_ffn = PositionwiseFeedForward(\n            d_model, d_inner, dropout=dropout, normalize_before=normalize_before)",
        "detail": "layers.Pyraformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "layers.Pyraformer_EncDec",
        "description": "layers.Pyraformer_EncDec",
        "peekOfCode": "class Encoder(nn.Module):\n    \"\"\" A encoder model with self attention mechanism. \"\"\"\n    def __init__(self, configs, window_size, inner_size):\n        super().__init__()\n        d_bottleneck = configs.d_model//4\n        self.mask, self.all_size = get_mask(\n            configs.seq_len, window_size, inner_size)\n        self.indexes = refer_points(self.all_size, window_size)\n        self.layers = nn.ModuleList([\n            EncoderLayer(configs.d_model, configs.d_ff, configs.n_heads, dropout=configs.dropout,",
        "detail": "layers.Pyraformer_EncDec",
        "documentation": {}
    },
    {
        "label": "ConvLayer",
        "kind": 6,
        "importPath": "layers.Pyraformer_EncDec",
        "description": "layers.Pyraformer_EncDec",
        "peekOfCode": "class ConvLayer(nn.Module):\n    def __init__(self, c_in, window_size):\n        super(ConvLayer, self).__init__()\n        self.downConv = nn.Conv1d(in_channels=c_in,\n                                  out_channels=c_in,\n                                  kernel_size=window_size,\n                                  stride=window_size)\n        self.norm = nn.BatchNorm1d(c_in)\n        self.activation = nn.ELU()\n    def forward(self, x):",
        "detail": "layers.Pyraformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Bottleneck_Construct",
        "kind": 6,
        "importPath": "layers.Pyraformer_EncDec",
        "description": "layers.Pyraformer_EncDec",
        "peekOfCode": "class Bottleneck_Construct(nn.Module):\n    \"\"\"Bottleneck convolution CSCM\"\"\"\n    def __init__(self, d_model, window_size, d_inner):\n        super(Bottleneck_Construct, self).__init__()\n        if not isinstance(window_size, list):\n            self.conv_layers = nn.ModuleList([\n                ConvLayer(d_inner, window_size),\n                ConvLayer(d_inner, window_size),\n                ConvLayer(d_inner, window_size)\n            ])",
        "detail": "layers.Pyraformer_EncDec",
        "documentation": {}
    },
    {
        "label": "PositionwiseFeedForward",
        "kind": 6,
        "importPath": "layers.Pyraformer_EncDec",
        "description": "layers.Pyraformer_EncDec",
        "peekOfCode": "class PositionwiseFeedForward(nn.Module):\n    \"\"\" Two-layer position-wise feed-forward neural network. \"\"\"\n    def __init__(self, d_in, d_hid, dropout=0.1, normalize_before=True):\n        super().__init__()\n        self.normalize_before = normalize_before\n        self.w_1 = nn.Linear(d_in, d_hid)\n        self.w_2 = nn.Linear(d_hid, d_in)\n        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):",
        "detail": "layers.Pyraformer_EncDec",
        "documentation": {}
    },
    {
        "label": "get_mask",
        "kind": 2,
        "importPath": "layers.Pyraformer_EncDec",
        "description": "layers.Pyraformer_EncDec",
        "peekOfCode": "def get_mask(input_size, window_size, inner_size):\n    \"\"\"Get the attention mask of PAM-Naive\"\"\"\n    # Get the size of all layers\n    all_size = []\n    all_size.append(input_size)\n    for i in range(len(window_size)):\n        layer_size = math.floor(all_size[i] / window_size[i])\n        all_size.append(layer_size)\n    seq_length = sum(all_size)\n    mask = torch.zeros(seq_length, seq_length)",
        "detail": "layers.Pyraformer_EncDec",
        "documentation": {}
    },
    {
        "label": "refer_points",
        "kind": 2,
        "importPath": "layers.Pyraformer_EncDec",
        "description": "layers.Pyraformer_EncDec",
        "peekOfCode": "def refer_points(all_sizes, window_size):\n    \"\"\"Gather features from PAM's pyramid sequences\"\"\"\n    input_size = all_sizes[0]\n    indexes = torch.zeros(input_size, len(all_sizes))\n    for i in range(input_size):\n        indexes[i][0] = i\n        former_index = i\n        for j in range(1, len(all_sizes)):\n            start = sum(all_sizes[:j])\n            inner_layer_idx = former_index - (start - all_sizes[j - 1])",
        "detail": "layers.Pyraformer_EncDec",
        "documentation": {}
    },
    {
        "label": "DSAttention",
        "kind": 6,
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "peekOfCode": "class DSAttention(nn.Module):\n    '''De-stationary Attention'''\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n        super(DSAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L, H, E = queries.shape",
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "FullAttention",
        "kind": 6,
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "peekOfCode": "class FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n        super(FullAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape",
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "ProbAttention",
        "kind": 6,
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "peekOfCode": "class ProbAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n        super(ProbAttention, self).__init__()\n        self.factor = factor\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n    def _prob_QK(self, Q, K, sample_k, n_top):  # n_top: c*ln(L_q)\n        # Q [B, H, L, D]",
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "AttentionLayer",
        "kind": 6,
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "peekOfCode": "class AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n                 d_values=None):\n        super(AttentionLayer, self).__init__()\n        d_keys = d_keys or (d_model // n_heads)\n        d_values = d_values or (d_model // n_heads)\n        self.inner_attention = attention\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.value_projection = nn.Linear(d_model, d_values * n_heads)",
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "ReformerLayer",
        "kind": 6,
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "peekOfCode": "class ReformerLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n                 d_values=None, causal=False, bucket_size=4, n_hashes=4):\n        super().__init__()\n        self.bucket_size = bucket_size\n        self.attn = LSHSelfAttention(\n            dim=d_model,\n            heads=n_heads,\n            bucket_size=bucket_size,\n            n_hashes=n_hashes,",
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "TwoStageAttentionLayer",
        "kind": 6,
        "importPath": "layers.SelfAttention_Family",
        "description": "layers.SelfAttention_Family",
        "peekOfCode": "class TwoStageAttentionLayer(nn.Module):\n    '''\n    The Two Stage Attention (TSA) Layer\n    input/output shape: [batch_size, Data_dim(D), Seg_num(L), d_model]\n    '''\n    def __init__(self, configs,\n                 seg_num, factor, d_model, n_heads, d_ff=None, dropout=0.1):\n        super(TwoStageAttentionLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.time_attention = AttentionLayer(FullAttention(False, configs.factor, attention_dropout=configs.dropout,",
        "detail": "layers.SelfAttention_Family",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 6,
        "importPath": "layers.StandardNorm",
        "description": "layers.StandardNorm",
        "peekOfCode": "class Normalize(nn.Module):\n    def __init__(self, num_features: int, eps=1e-5, affine=False, subtract_last=False, non_norm=False):\n        \"\"\"\n        :param num_features: the number of features or channels\n        :param eps: a value added for numerical stability\n        :param affine: if True, RevIN has learnable affine parameters\n        \"\"\"\n        super(Normalize, self).__init__()\n        self.num_features = num_features\n        self.eps = eps",
        "detail": "layers.StandardNorm",
        "documentation": {}
    },
    {
        "label": "ConvLayer",
        "kind": 6,
        "importPath": "layers.Transformer_EncDec",
        "description": "layers.Transformer_EncDec",
        "peekOfCode": "class ConvLayer(nn.Module):\n    def __init__(self, c_in):\n        super(ConvLayer, self).__init__()\n        self.downConv = nn.Conv1d(in_channels=c_in,\n                                  out_channels=c_in,\n                                  kernel_size=3,\n                                  padding=2,\n                                  padding_mode='circular')\n        self.norm = nn.BatchNorm1d(c_in)\n        self.activation = nn.ELU()",
        "detail": "layers.Transformer_EncDec",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "layers.Transformer_EncDec",
        "description": "layers.Transformer_EncDec",
        "peekOfCode": "class EncoderLayer(nn.Module):\n    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)",
        "detail": "layers.Transformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "layers.Transformer_EncDec",
        "description": "layers.Transformer_EncDec",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(Encoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n    def forward(self, x, attn_mask=None, tau=None, delta=None):\n        # x [B, L, D]\n        attns = []\n        if self.conv_layers is not None:",
        "detail": "layers.Transformer_EncDec",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "layers.Transformer_EncDec",
        "description": "layers.Transformer_EncDec",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n                 dropout=0.1, activation=\"relu\"):\n        super(DecoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.self_attention = self_attention\n        self.cross_attention = cross_attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)",
        "detail": "layers.Transformer_EncDec",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "layers.Transformer_EncDec",
        "description": "layers.Transformer_EncDec",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, layers, norm_layer=None, projection=None):\n        super(Decoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n        self.projection = projection\n    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n        for layer in self.layers:\n            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask, tau=tau, delta=delta)\n        if self.norm is not None:",
        "detail": "layers.Transformer_EncDec",
        "documentation": {}
    },
    {
        "label": "PositionalEmbedding",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class PositionalEmbedding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model).float()\n        pe.require_grad = False\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float()\n                    * -(math.log(10000.0) / d_model)).exp()\n        pe[:, 0::2] = torch.sin(position * div_term)",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "TokenEmbedding",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class TokenEmbedding(nn.Module):\n    def __init__(self, c_in, d_model):\n        super(TokenEmbedding, self).__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_in', nonlinearity='leaky_relu')",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "FixedEmbedding",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class FixedEmbedding(nn.Module):\n    def __init__(self, c_in, d_model):\n        super(FixedEmbedding, self).__init__()\n        w = torch.zeros(c_in, d_model).float()\n        w.require_grad = False\n        position = torch.arange(0, c_in).float().unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float()\n                    * -(math.log(10000.0) / d_model)).exp()\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "TemporalEmbedding",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class TemporalEmbedding(nn.Module):\n    def __init__(self, d_model, embed_type='fixed', freq='h'):\n        super(TemporalEmbedding, self).__init__()\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "TimeFeatureEmbedding",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class TimeFeatureEmbedding(nn.Module):\n    def __init__(self, d_model, embed_type='timeF', freq='h'):\n        super(TimeFeatureEmbedding, self).__init__()\n        freq_map = {'h': 4, 't': 5, 's': 6,\n                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n        d_inp = freq_map[freq]\n        self.embed = nn.Linear(d_inp, d_model, bias=False)\n    def forward(self, x):\n        return self.embed(x)\nclass DataEmbedding(nn.Module):",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "DataEmbedding",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class DataEmbedding(nn.Module):\n    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n        super(DataEmbedding, self).__init__()\n        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n        self.position_embedding = PositionalEmbedding(d_model=d_model)\n        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n            d_model=d_model, embed_type=embed_type, freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n    def forward(self, x, x_mark):",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "moving_avg",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class moving_avg(nn.Module):\n    \"\"\"\n    Moving average block to highlight the trend of time series\n    \"\"\"\n    def __init__(self, kernel_size, stride):\n        super(moving_avg, self).__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n    def forward(self, x):\n        # padding on the both ends of time series",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "series_decomp",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class series_decomp(nn.Module):\n    \"\"\"\n    Series decomposition block\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(series_decomp, self).__init__()\n        self.moving_avg = moving_avg(kernel_size, stride=1)\n    def forward(self, x):\n        moving_mean = self.moving_avg(x)\n        res = x - moving_mean",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "series_decomp_multi",
        "kind": 6,
        "importPath": "layers.layers",
        "description": "layers.layers",
        "peekOfCode": "class series_decomp_multi(nn.Module):\n    \"\"\"\n    Multiple Series decomposition block from FEDformer\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(series_decomp_multi, self).__init__()\n        self.kernel_size = kernel_size\n        self.series_decomp = [series_decomp(kernel) for kernel in kernel_size]\n    def forward(self, x):\n        moving_mean = []",
        "detail": "layers.layers",
        "documentation": {}
    },
    {
        "label": "MIC",
        "kind": 6,
        "importPath": "models.MICN",
        "description": "models.MICN",
        "peekOfCode": "class MIC(nn.Module):\n    \"\"\"\n    MIC layer to extract local and global features\n    \"\"\"\n    def __init__(self, feature_size=512, n_heads=8, dropout=0.05, decomp_kernel=[32], conv_kernel=[24],\n                 isometric_kernel=[18, 6], device='cuda'):\n        super(MIC, self).__init__()\n        self.conv_kernel = conv_kernel\n        self.device = device\n        # isometric convolution",
        "detail": "models.MICN",
        "documentation": {}
    },
    {
        "label": "SeasonalPrediction",
        "kind": 6,
        "importPath": "models.MICN",
        "description": "models.MICN",
        "peekOfCode": "class SeasonalPrediction(nn.Module):\n    def __init__(self, embedding_size=512, n_heads=8, dropout=0.05, d_layers=1, decomp_kernel=[32], c_out=1,\n                 conv_kernel=[2, 4], isometric_kernel=[18, 6], device='cuda'):\n        super(SeasonalPrediction, self).__init__()\n        self.mic = nn.ModuleList([MIC(feature_size=embedding_size, n_heads=n_heads,\n                                      decomp_kernel=decomp_kernel, conv_kernel=conv_kernel,\n                                      isometric_kernel=isometric_kernel, device=device)\n                                  for i in range(d_layers)])\n        self.projection = nn.Linear(embedding_size, c_out)\n    def forward(self, dec):",
        "detail": "models.MICN",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 6,
        "importPath": "models.MICN",
        "description": "models.MICN",
        "peekOfCode": "class Model(nn.Module):\n    \"\"\"\n    Paper link: https://openreview.net/pdf?id=zt53IDUR1U\n    \"\"\"\n    def __init__(self, configs, conv_kernel=[12, 16]):\n        \"\"\"\n        conv_kernel: downsampling and upsampling convolution kernel_size\n        \"\"\"\n        super(Model, self).__init__()\n        # Initialize lists for the kernels of the decomposition operation and isometric convolution",
        "detail": "models.MICN",
        "documentation": {}
    },
    {
        "label": "mape_loss",
        "kind": 6,
        "importPath": "utils.losses",
        "description": "utils.losses",
        "peekOfCode": "class mape_loss(nn.Module):\n    def __init__(self):\n        super(mape_loss, self).__init__()\n    def forward(self, insample: t.Tensor, freq: int,\n                forecast: t.Tensor, target: t.Tensor, mask: t.Tensor) -> t.float:\n        \"\"\"\n        MAPE loss as defined in: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\n        :param forecast: Forecast values. Shape: batch, time\n        :param target: Target values. Shape: batch, time\n        :param mask: 0/1 mask. Shape: batch, time",
        "detail": "utils.losses",
        "documentation": {}
    },
    {
        "label": "smape_loss",
        "kind": 6,
        "importPath": "utils.losses",
        "description": "utils.losses",
        "peekOfCode": "class smape_loss(nn.Module):\n    def __init__(self):\n        super(smape_loss, self).__init__()\n    def forward(self, insample: t.Tensor, freq: int,\n                forecast: t.Tensor, target: t.Tensor, mask: t.Tensor) -> t.float:\n        \"\"\"\n        sMAPE loss as defined in https://robjhyndman.com/hyndsight/smape/ (Makridakis 1993)\n        :param forecast: Forecast values. Shape: batch, time\n        :param target: Target values. Shape: batch, time\n        :param mask: 0/1 mask. Shape: batch, time",
        "detail": "utils.losses",
        "documentation": {}
    },
    {
        "label": "mase_loss",
        "kind": 6,
        "importPath": "utils.losses",
        "description": "utils.losses",
        "peekOfCode": "class mase_loss(nn.Module):\n    def __init__(self):\n        super(mase_loss, self).__init__()\n    def forward(self, insample: t.Tensor, freq: int,\n                forecast: t.Tensor, target: t.Tensor, mask: t.Tensor) -> t.float:\n        \"\"\"\n        MASE loss as defined in \"Scaled Errors\" https://robjhyndman.com/papers/mase.pdf\n        :param insample: Insample values. Shape: batch, time_i\n        :param freq: Frequency value\n        :param forecast: Forecast values. Shape: batch, time_o",
        "detail": "utils.losses",
        "documentation": {}
    },
    {
        "label": "divide_no_nan",
        "kind": 2,
        "importPath": "utils.losses",
        "description": "utils.losses",
        "peekOfCode": "def divide_no_nan(a, b):\n    \"\"\"\n    a/b where the resulted NaN or Inf are replaced by 0.\n    \"\"\"\n    result = a / b\n    result[result != result] = .0\n    result[result == np.inf] = .0\n    return result\nclass mape_loss(nn.Module):\n    def __init__(self):",
        "detail": "utils.losses",
        "documentation": {}
    },
    {
        "label": "M4Summary",
        "kind": 6,
        "importPath": "utils.m4_summary",
        "description": "utils.m4_summary",
        "peekOfCode": "class M4Summary:\n    def __init__(self, file_path, root_path):\n        self.file_path = file_path\n        self.training_set = M4Dataset.load(training=True, dataset_file=root_path)\n        self.test_set = M4Dataset.load(training=False, dataset_file=root_path)\n        self.naive_path = os.path.join(root_path, 'submission-Naive2.csv')\n    def evaluate(self):\n        \"\"\"\n        Evaluate forecasts using M4 test dataset.\n        :param forecast: Forecasts. Shape: timeseries, time.",
        "detail": "utils.m4_summary",
        "documentation": {}
    },
    {
        "label": "group_values",
        "kind": 2,
        "importPath": "utils.m4_summary",
        "description": "utils.m4_summary",
        "peekOfCode": "def group_values(values, groups, group_name):\n    return np.array([v[~np.isnan(v)] for v in values[groups == group_name]])\ndef mase(forecast, insample, outsample, frequency):\n    return np.mean(np.abs(forecast - outsample)) / np.mean(np.abs(insample[:-frequency] - insample[frequency:]))\ndef smape_2(forecast, target):\n    denom = np.abs(target) + np.abs(forecast)\n    # divide by 1.0 instead of 0.0, in case when denom is zero the enumerator will be 0.0 anyway.\n    denom[denom == 0.0] = 1.0\n    return 200 * np.abs(forecast - target) / denom\ndef mape(forecast, target):",
        "detail": "utils.m4_summary",
        "documentation": {}
    },
    {
        "label": "mase",
        "kind": 2,
        "importPath": "utils.m4_summary",
        "description": "utils.m4_summary",
        "peekOfCode": "def mase(forecast, insample, outsample, frequency):\n    return np.mean(np.abs(forecast - outsample)) / np.mean(np.abs(insample[:-frequency] - insample[frequency:]))\ndef smape_2(forecast, target):\n    denom = np.abs(target) + np.abs(forecast)\n    # divide by 1.0 instead of 0.0, in case when denom is zero the enumerator will be 0.0 anyway.\n    denom[denom == 0.0] = 1.0\n    return 200 * np.abs(forecast - target) / denom\ndef mape(forecast, target):\n    denom = np.abs(target)\n    # divide by 1.0 instead of 0.0, in case when denom is zero the enumerator will be 0.0 anyway.",
        "detail": "utils.m4_summary",
        "documentation": {}
    },
    {
        "label": "smape_2",
        "kind": 2,
        "importPath": "utils.m4_summary",
        "description": "utils.m4_summary",
        "peekOfCode": "def smape_2(forecast, target):\n    denom = np.abs(target) + np.abs(forecast)\n    # divide by 1.0 instead of 0.0, in case when denom is zero the enumerator will be 0.0 anyway.\n    denom[denom == 0.0] = 1.0\n    return 200 * np.abs(forecast - target) / denom\ndef mape(forecast, target):\n    denom = np.abs(target)\n    # divide by 1.0 instead of 0.0, in case when denom is zero the enumerator will be 0.0 anyway.\n    denom[denom == 0.0] = 1.0\n    return 100 * np.abs(forecast - target) / denom",
        "detail": "utils.m4_summary",
        "documentation": {}
    },
    {
        "label": "mape",
        "kind": 2,
        "importPath": "utils.m4_summary",
        "description": "utils.m4_summary",
        "peekOfCode": "def mape(forecast, target):\n    denom = np.abs(target)\n    # divide by 1.0 instead of 0.0, in case when denom is zero the enumerator will be 0.0 anyway.\n    denom[denom == 0.0] = 1.0\n    return 100 * np.abs(forecast - target) / denom\nclass M4Summary:\n    def __init__(self, file_path, root_path):\n        self.file_path = file_path\n        self.training_set = M4Dataset.load(training=True, dataset_file=root_path)\n        self.test_set = M4Dataset.load(training=False, dataset_file=root_path)",
        "detail": "utils.m4_summary",
        "documentation": {}
    },
    {
        "label": "TriangularCausalMask",
        "kind": 6,
        "importPath": "utils.masking",
        "description": "utils.masking",
        "peekOfCode": "class TriangularCausalMask():\n    def __init__(self, B, L, device=\"cpu\"):\n        mask_shape = [B, 1, L, L]\n        with torch.no_grad():\n            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n    @property\n    def mask(self):\n        return self._mask\nclass ProbMask():\n    def __init__(self, B, H, L, index, scores, device=\"cpu\"):",
        "detail": "utils.masking",
        "documentation": {}
    },
    {
        "label": "ProbMask",
        "kind": 6,
        "importPath": "utils.masking",
        "description": "utils.masking",
        "peekOfCode": "class ProbMask():\n    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n        indicator = _mask_ex[torch.arange(B)[:, None, None],\n                    torch.arange(H)[None, :, None],\n                    index, :].to(device)\n        self._mask = indicator.view(scores.shape).to(device)\n    @property\n    def mask(self):",
        "detail": "utils.masking",
        "documentation": {}
    },
    {
        "label": "RSE",
        "kind": 2,
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "peekOfCode": "def RSE(pred, true):\n    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean()) ** 2))\ndef CORR(pred, true):\n    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)\n    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))\n    return (u / d).mean(-1)\ndef MAE(pred, true):\n    return np.mean(np.abs(pred - true))\ndef MSE(pred, true):\n    return np.mean((pred - true) ** 2)",
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "CORR",
        "kind": 2,
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "peekOfCode": "def CORR(pred, true):\n    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)\n    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))\n    return (u / d).mean(-1)\ndef MAE(pred, true):\n    return np.mean(np.abs(pred - true))\ndef MSE(pred, true):\n    return np.mean((pred - true) ** 2)\ndef RMSE(pred, true):\n    return np.sqrt(MSE(pred, true))",
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "MAE",
        "kind": 2,
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "peekOfCode": "def MAE(pred, true):\n    return np.mean(np.abs(pred - true))\ndef MSE(pred, true):\n    return np.mean((pred - true) ** 2)\ndef RMSE(pred, true):\n    return np.sqrt(MSE(pred, true))\ndef MAPE(pred, true):\n    return np.mean(np.abs((pred - true) / true))\ndef MSPE(pred, true):\n    return np.mean(np.square((pred - true) / true))",
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "MSE",
        "kind": 2,
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "peekOfCode": "def MSE(pred, true):\n    return np.mean((pred - true) ** 2)\ndef RMSE(pred, true):\n    return np.sqrt(MSE(pred, true))\ndef MAPE(pred, true):\n    return np.mean(np.abs((pred - true) / true))\ndef MSPE(pred, true):\n    return np.mean(np.square((pred - true) / true))\ndef metric(pred, true):\n    mae = MAE(pred, true)",
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "RMSE",
        "kind": 2,
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "peekOfCode": "def RMSE(pred, true):\n    return np.sqrt(MSE(pred, true))\ndef MAPE(pred, true):\n    return np.mean(np.abs((pred - true) / true))\ndef MSPE(pred, true):\n    return np.mean(np.square((pred - true) / true))\ndef metric(pred, true):\n    mae = MAE(pred, true)\n    mse = MSE(pred, true)\n    rmse = RMSE(pred, true)",
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "MAPE",
        "kind": 2,
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "peekOfCode": "def MAPE(pred, true):\n    return np.mean(np.abs((pred - true) / true))\ndef MSPE(pred, true):\n    return np.mean(np.square((pred - true) / true))\ndef metric(pred, true):\n    mae = MAE(pred, true)\n    mse = MSE(pred, true)\n    rmse = RMSE(pred, true)\n    mape = MAPE(pred, true)\n    mspe = MSPE(pred, true)",
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "MSPE",
        "kind": 2,
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "peekOfCode": "def MSPE(pred, true):\n    return np.mean(np.square((pred - true) / true))\ndef metric(pred, true):\n    mae = MAE(pred, true)\n    mse = MSE(pred, true)\n    rmse = RMSE(pred, true)\n    mape = MAPE(pred, true)\n    mspe = MSPE(pred, true)\n    return mae, mse, rmse, mape, mspe",
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "metric",
        "kind": 2,
        "importPath": "utils.metrics",
        "description": "utils.metrics",
        "peekOfCode": "def metric(pred, true):\n    mae = MAE(pred, true)\n    mse = MSE(pred, true)\n    rmse = RMSE(pred, true)\n    mape = MAPE(pred, true)\n    mspe = MSPE(pred, true)\n    return mae, mse, rmse, mape, mspe",
        "detail": "utils.metrics",
        "documentation": {}
    },
    {
        "label": "print_args",
        "kind": 2,
        "importPath": "utils.print_args",
        "description": "utils.print_args",
        "peekOfCode": "def print_args(args):\n    print(\"\\033[1m\" + \"Basic Config\" + \"\\033[0m\")\n    print(f'  {\"Task Name:\":<20}{args.task_name:<20}{\"Is Training:\":<20}{args.is_training:<20}')\n    print(f'  {\"Model ID:\":<20}{args.model_id:<20}{\"Model:\":<20}{args.model:<20}')\n    print()\n    print(\"\\033[1m\" + \"Data Loader\" + \"\\033[0m\")\n    print(f'  {\"Data:\":<20}{args.data:<20}{\"Root Path:\":<20}{args.root_path:<20}')\n    print(f'  {\"Data Path:\":<20}{args.data_path:<20}{\"Features:\":<20}{args.features:<20}')\n    print(f'  {\"Target:\":<20}{args.target:<20}{\"Freq:\":<20}{args.freq:<20}')\n    print(f'  {\"Checkpoints:\":<20}{args.checkpoints:<20}')",
        "detail": "utils.print_args",
        "documentation": {}
    },
    {
        "label": "TimeFeature",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class TimeFeature:\n    def __init__(self):\n        pass\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        pass\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "SecondOfMinute",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "MinuteOfHour",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "HourOfDay",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "DayOfWeek",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "DayOfMonth",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "DayOfYear",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "MonthOfYear",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "WeekOfYear",
        "kind": 6,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "class WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.isocalendar().week - 1) / 52.0 - 0.5\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "time_features_from_frequency_str",
        "kind": 2,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n    features_by_offsets = {\n        offsets.YearEnd: [],",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "time_features",
        "kind": 2,
        "importPath": "utils.timefeatures",
        "description": "utils.timefeatures",
        "peekOfCode": "def time_features(dates, freq='h'):\n    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])",
        "detail": "utils.timefeatures",
        "documentation": {}
    },
    {
        "label": "EarlyStopping",
        "kind": 6,
        "importPath": "utils.tools",
        "description": "utils.tools",
        "peekOfCode": "class EarlyStopping:\n    def __init__(self, patience=7, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n    def __call__(self, val_loss, model, path):",
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "dotdict",
        "kind": 6,
        "importPath": "utils.tools",
        "description": "utils.tools",
        "peekOfCode": "class dotdict(dict):\n    \"\"\"dot.notation access to dictionary attributes\"\"\"\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\nclass StandardScaler():\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def transform(self, data):",
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "kind": 6,
        "importPath": "utils.tools",
        "description": "utils.tools",
        "peekOfCode": "class StandardScaler():\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n    def transform(self, data):\n        return (data - self.mean) / self.std\n    def inverse_transform(self, data):\n        return (data * self.std) + self.mean\ndef visual(true, preds=None, name='./pic/test.pdf'):\n    \"\"\"",
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "adjust_learning_rate",
        "kind": 2,
        "importPath": "utils.tools",
        "description": "utils.tools",
        "peekOfCode": "def adjust_learning_rate(optimizer, epoch, args):\n    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n    if args.lradj == 'type1':\n        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n    elif args.lradj == 'type2':\n        lr_adjust = {\n            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n            10: 5e-7, 15: 1e-7, 20: 5e-8\n        }\n    elif args.lradj == \"cosine\":",
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "visual",
        "kind": 2,
        "importPath": "utils.tools",
        "description": "utils.tools",
        "peekOfCode": "def visual(true, preds=None, name='./pic/test.pdf'):\n    \"\"\"\n    Results visualization\n    \"\"\"\n    plt.figure()\n    plt.plot(true, label='GroundTruth', linewidth=2)\n    if preds is not None:\n        plt.plot(preds, label='Prediction', linewidth=2)\n    plt.legend()\n    plt.savefig(name, bbox_inches='tight')",
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "adjustment",
        "kind": 2,
        "importPath": "utils.tools",
        "description": "utils.tools",
        "peekOfCode": "def adjustment(gt, pred):\n    anomaly_state = False\n    for i in range(len(gt)):\n        if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n            anomaly_state = True\n            for j in range(i, 0, -1):\n                if gt[j] == 0:\n                    break\n                else:\n                    if pred[j] == 0:",
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "cal_accuracy",
        "kind": 2,
        "importPath": "utils.tools",
        "description": "utils.tools",
        "peekOfCode": "def cal_accuracy(y_pred, y_true):\n    return np.mean(y_pred == y_true)",
        "detail": "utils.tools",
        "documentation": {}
    }
]